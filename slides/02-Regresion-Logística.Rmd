---
# title: "{ingemat}§2e|0l|2e|2m|2e|1n|0[-]t|1o|3[++]s|2  d|2s|2{202201}[26°c[10°c]"
title: "Intro Machine Learning"
pagetitle: "02-Regresión-Logística"
subtitle: "02 Regresión Logística"
author: "<br>Joshua Kunst<br>@jbkunst"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    self_contained: no
    css: ["css/xaringan-themer.css", "css/styles.css"]
    lib_dir: libs    
    nature:
      titleSlideClass: ["left", "middle"]
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
source(here::here("R/00-xaringan-knitr-setup.R"))
```

## Regresión Logística (cont. Clasificación)

Si bien el método se llama _Regresión Logística_ es un modelo que se
utilza para clasificar respuestas **binarias**. Esto debido a su forma:

$$p_{i}={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x_{1,i}+\cdots +\beta _{k}x_{k,i})}}}$$

Donde $L(X_i) = \beta _{0}+\beta _{1}x_{1,i}+\cdots +\beta _{k}x_{k,i}$ es la combinación lineal de los predictores de la observación $i$.


---

## Regresión Lineal (ejercicios I)

- Calcule el límite de los valores de $p_i$ dependiendo si $L(X_i)$ toma valores muy grandes o muy pequeños.
- Grafique en R la función $f_1(x)={\frac {1}{1+e^{-x}}}$ y la function $f_2(x)={\frac {1}{1+e^{-(5 + 8x)}}}$.



---

## Regresión Lineal (ejercicios II)


Lea los datos de la clase anterior:

1. Grafique un diagrama de puntos utilizando las variables $x = elevation$ e $y = in_sf$

1. Utilizando la intuición dibuje sobre el gráfico anterior alguna función de la forma $f(x)={\frac {1}{1+e^{-(\beta_0 + \beta_1 elevation)}}}$ escogiendo los "mejores" $(\beta_0, \beta_1)$.

1. Genere una grilla de valores $(\beta_0, \beta_1)$ y evalue alguna métrica de ajuste para encontrar
los "mejores" $(\beta_0, \beta_1)$.

1. Grafique un diagrama de puntos utilizando las variables $x = elevation$ e $y = price$ y coloreando
por $z = in_sf$. 


---

## Intuición de ajuste de Regresión Logístca

```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.height = 3,
  fig.width  = 9,
  echo = FALSE
)
```


```{r}
library(tidyverse)

plot_rl_mod <- function(df){
  
  # df <- purrr::rerun(100, df) |> 
  #   bind_rows()
  
  mod <- glm(response ~ x, data = df, family = "binomial")
  
  summary(mod)
  
  df2 <- tibble(x = seq(min(df$x), max(df$x), length.out = 100))
  df2 <- df2 |> 
    mutate(
      response = predict(mod, newdata = df2, type = "response")
    )
  
  ggplot(mapping = aes(x, response)) +
    geom_point(aes(color = factor(response)), data = mod$data, name = NULL) +
    geom_line(data = df2, color = "gray") +
    scale_color_viridis_d(begin = 0.1, end = 0.9, name = NULL) +
    theme(legend.position = "right")
  
}

tibble_rl_mod <- function(df){
  
  mod <- glm(response ~ x, data = df, family = "binomial")
  
  summary(mod)
  
  fun_sep <- function(x){
    # x |> 
    #   map(str_split, "") |> 
    #   map(unlist) |> 
    #   map_chr(str_c, collapse = " ") 
    str_replace_all(x, "\\*", "†")
  }
  
  broom::tidy(mod) |> 
    mutate(`Signif` = fun_sep(modrpley::signif_star(p.value)))
  
}

df0 <- tibble(
  x        = c(1, 2, 3, 4),
  response = c(0, 0, 1, 1)
)

df <- df0

plot_rl_mod(df)

knitr::kable(tibble_rl_mod(df))
```


---

## Intuición de ajuste de Regresión Logístca (cont 2)

```{r}
df <- add_row(df, x = 2, response = 1)
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición de ajuste de Regresión Logístca (cont 3)

```{r}
df <- add_row(df, x = 1, response = 1)
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición de ajuste de Regresión Logístca (cont 4)

```{r}
df <- add_row(df, x = 3, response = 0)
df <- add_row(df, x = 4, response = 0)
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición de ajuste de Regresión Logístca (cont 5)

```{r}
df <- df0 |>
  mutate(x = rev(x)) 
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición de ajuste de Regresión Logístca (cont 6)

```{r}
df <- df0 |>
  mutate(x = 1000 * x) 
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición de ajuste de Regresión Logístca (cont 7)

```{r}
df <- df0 |>
  mutate(x =  x/1000) 
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición Regresión Logístca (cont 8) elevation

```{r}
ruta <- "https://raw.githubusercontent.com/jadeyee/r2d3-part-1-data/master/part_1_data.csv"
datos <- readr::read_csv(ruta, skip = 2)

df <- datos |>
  select(x = elevation, response = in_sf)
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```

---

## Intuición Regresión Logístca (cont 9) sqft

```{r}
df <- datos |>
  select(x = sqft, response = in_sf)
plot_rl_mod(df)
knitr::kable(tibble_rl_mod(df))
```


---

## Ejercicios II


1. En los datos del repositorio lea los datos de crédto de entrenamiento y
valdación (train/test si le gusta el _ingelishon_).

1. Encuentre el mejor modelo de regresión logística con 4 variables. Use su intuición
sobre lo que intuye/conoce del _negocio_.

1. Que modelo (árbol/regresión logística) performa mejor en la muestra de validación?
